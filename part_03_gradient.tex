% !TEX root = main.tex

%%%-------------------------------------------
\section{Пятьдесят оттенков градиентного спуска}

\epigraph{Повторять до сходимости --- это как жарить до готовности}{\textit{Неизвестный студент Вышки}}

% \epigraph{Производная это просто \newline Скорость роста, это скорость роста. \newline Возьми предел $\frac{\Delta y}{\Delta x}$ и получишь. \newline Чем выше она --- тем круче.}{\textit{Научно-технический рэп}}

\begin{problem}{(50 оттенков спуска)}
	Маша Нестерова, хозяйка машин лёрнинга\footnote{Лёрнинг ей папа подарил},  собрала два наблюдения: $x_1 = 1, x_2 = 2$, $y_1 = 2, y_2 = 3$ и собирается обучить линейную регрессию $y = w \cdot x$.  Маша очень хрупкая девушка, и ей не помешает помощь. 

	\begin{enumerate}
		\item Получите теоретическую оценку методом наименьших квадратов.
		
		\item  Сделайте два шага градиентного спуска. В качестве стартовой точки используйте $w_0 = 0$.  В качестве скорости обучения возьмите $\eta = 0.1$. 
		
		\item Сделайте два шага стохастического градиентного спуска.  Пусть в SGD сначала попадает первое наблюдение, затем второе. 
		
		\item Если вы добрались до этого пункта, вы поняли градиентный спуск. Маша довольна. Начинаем заниматься тупой технической бессмыслицей. Сделайте два шага Momentum SGD. Возьмите $\alpha = 0.9, \eta = 0.1$
		
		\item  Сделайте два шага Momentum SGD с коррекцией Нестерова. 
		
		\item Сделайте два шага RMSprop.  Возьмите $\alpha = 0.9, \eta = 0.1$
		
		\item  Сделайте два шага Adam. Возьмём  $\beta_1 = \beta_2 = 0.9, \eta = 0.1$		
	\end{enumerate}
\end{problem}



%%%%-------------------------------------------
\begin{problem}{(логистическая регрессия)}
Маша решила, что нет смысла останавливаться на обычной регрессии, когда она знает, что есть ещё и логистическая:

\begin{equation*}
\begin{aligned}
& z  = w \cdot x \qquad p = P(y = 1) = \frac{1}{1 + e^{-z}} \\
& \logloss = -[ y \cdot \ln p + (1 - y) \cdot \ln (1 - p) ]
\end{aligned}
\end{equation*}

Запишите формулу, по которой можно пересчитывать веса в ходе градиентного спуска для логистической регрессии. 

Оказалось, что $x = -5$, а $y = 1$. Сделайте один шаг градиентного спуска, если $w_0 = 1$, а скорость обучения $\gamma = 0.01$. 
\end{problem}

\begin{sol}
Сначала нам надо найти $\logloss'_{\beta}$. В принципе в этом и заключается вся сложность задачки. Давайте подставим вместо $\hat p $ в $\logloss$ сигмоиду. 

$$
\logloss = -1 \left (y \cdot \ln \left( \frac{1}{1 + e^{-z}} \right)  + (1 - y) \cdot \ln \left ( 1 - \frac{1}{1 + e^{-z}} \right ) \right)
$$
	
Теперь подставим вместо $z$ уравнение регрессии:
	

$$
\logloss = -1 \left (y \cdot \ln \left( \frac{1}{1 + e^{-w \cdot x}} \right)  + (1 - y) \cdot \ln \left ( 1 - \frac{1}{1 + e^{- w \cdot x}} \right ) \right)
$$

Это и есть наша функция потерь.  От неё нам нужно найти производную. Давайте подготовимся. 

\indef{Делай раз,} найдём производную $\logloss$ по $\hat p$: 

$$
\logloss'_{\hat p} = -1 \left(y \cdot \frac{1}{\hat p} - (1 - y) \cdot \frac{1}{(1 - p)} \right)
$$

\indef{Делай два,} найдём производную $\frac{1}{1 + e^{-w x}} $ по $w$: 

\begin{multline*}
\left(  \frac{1}{1 + e^{-w x}}   \right)'_{w}  = - \frac{1}{(1 + e^{-w x})^2} \cdot e^{-w x} \cdot (-x) =\frac{1}{1 + e^{-w x}}  \cdot \frac{e^{-w x}}{1 + e^{-w x}} \cdot x  = \\ = \frac{1}{1 + e^{-w x}}  \cdot  \left(1 - \frac{1}{1 + e^{-w x}}  \right) \cdot x
\end{multline*}

По-другому это можно записать как $\hat p \cdot (1 - \hat p) \cdot x$.  

\indef{Делай три, находим полную производную:}

\begin{multline*}
\logloss'_{\beta} = -1 \left(y \cdot \frac{1}{\hat p}  \cdot  \hat p \cdot  \left(1 - \hat p)  \right) \cdot x  - (1 - y) \cdot \frac{1}{(1 - \hat p)} \cdot  \hat p \cdot  \left(1 - \hat p)  \right) \cdot x \right) = \\ =  -y \cdot \left( 1 - \hat p \right) \cdot x + (1 - y) \cdot  \hat p  \cdot x =  (-y + y \hat p  + \hat p - y \hat p ) \cdot x = (\hat p - y) \cdot x
\end{multline*}
	
Найдём значение производной в точке $w_0 = 1$ для нашего наблюдения $x = -5, y=1$: 

$$
\left(\frac{1}{1 + e^{-1 \cdot (-5)}}  - 1 \right) \cdot (-5)  \approx  4.96
$$

Делаем шаг градиентного спуска: 

$$
w_1 = 1 - 0.01 \cdot 4.96 \approx 0.95
$$	
\end{sol} 


% \begin{problem}{(скорости обучения)}
% В стохастическом градиентном спуске веса изменяются по формуле

% \[
% w_t = w_{t-1} - \eta_t \cdot \nabla L(w_{t-1}, x_i, y_i),
% \] где наблюдение $i$ выбрано случайно. 

% Скорость обучения зависит от номера итерации. Для сходимости алгоритма ряд из скоростей $\sum_{t=0}^{\infty} \eta_t$ должен расходиться, а ряд $\sum_{t=0}^{\infty} \eta_t^2$ сходиться. То есть скорость спуска должна падать не слишком медленно, но и не слишком быстро. Какие из последовательностей, перечисленных ниже, можно использовать для алгоритма? 

% \begin{enumerate} 
%     \item $\eta_t = \frac{1}{t}$
%     \item $\eta_t = \frac{0.1}{t^{0.3}}$
%     \item $\eta_t = \frac{1}{\sqrt{t}}$
%     \item $\eta_t = \frac{1}{t^2}$
%     \item $\eta_t = \frac{1}{t^2}$
% \end{enumerate} 
% \end{problem} 



