% !TEX root = main.tex

%%%-------------------------------------------
\section{Регуляризация}

\epigraph{Цитата про переобучение}{Автор цитаты}


\begin{problem}{(Маша и покемоны)}
 Маша измерила вес трёх покемонов,  $y_1=6$, $y_2=6$, $y_3=10$.  Она хочет спрогнозировать вес следующего покемона. Модель для веса покемонов у Маши очень простая, $y_i = \beta + \varepsilon_i$, поэтому прогнозирует Маша по формуле $\hat y_i = \hat \beta$.
	
	Для оценки параметра $\beta$ Маша использует следующую целевую функцию:
	
	$$
	\sum (y_i - \hat \beta)^2 + \lambda \cdot \hat \beta^2
	$$
	
	\begin{enumerate}
		\item[a)] Найдите оптимальное $\hat \beta$ при $\lambda =0$.
		\item[б)] Найдите оптимальное $\hat \beta$ при произвольном $\lambda$. Правда ли, что чем больше $\lambda$, тем меньше $\beta$? 
		\item[в)] Подберите оптимальное $\lambda$ с помощью кросс-валидации leave one out («выкинь одного»). При такой валидации на первом шаге мы оцениваем модель на всей выборке без первого наблюдения, а на первом тестируем её. На втором шаге мы оцениваем модель на всей выборке без второго наблюдения, а на втором тестируем её. И так далее $n$ раз. Каждое наблюдение является отдельным фолдом.
		\item[г)] Найдите оптимальное $\hat \beta$ при $\lambda_{CV}$.
	\end{enumerate}
\end{problem}

\begin{problem}{(а вот и моя остановочка)}
    \todo[inline]{Сделать задачу по связи ранней остановки и регуляризатора. Как в книжке про диплернинг}
\end{problem}


\begin{problem}{(дропаут)}
Маша собирается обучить нейронную сеть для решения задачи регрессии, На вход в неё идёт $12$ переменных, в сетке есть $3$ скрытых слоя. В пером слое $300$ нейронов, во втором $200$, в третьем $100$. 

	\begin{enumerate}
		\item[a)] Сколько параметров предстоит оценить Маше?  Сколько наблюдений вы бы на её месте использовали? 
		\item[b)] Пусть в каждом слое была отключена половина нейронов. Сколько коэффициентов необходимо оценить?
		\item[c)] Предположим, что Маша решила после первого слоя добавить в свою сетку Dropout c вероятностью $p$.  Какова вероятность того, что отключится весь слой? 
		\item[d)] Маша добавила Dropout c вероятностью $p$. после каждого слоя. Какова вероятность того, что один из слоёв отключится и сетка не сможет учиться? 
		\item[e)] Пусть случайная величина $N$ --- это число включённых нейронов. Найдите её математическое ожидание и дисперсию. Если Маша хочет проредить сетку на четверть, какое значение $p$ она должна поставить? 
		\item[f)] Пусть случайная величина $P$ --- это число параметров в нейросети, которое необходимо оценить. Найдите её математическое ожидание и дисперсию. Почему найденное вами математическое ожидание выглядит очень логично? Что оно вам напоминает? Обратите внимание на то, что смерть одного из параметров легко может привести к смерти другого.
	\end{enumerate}
\end{problem}

\todo[inline]{Бэкпроп через дропаут}

\todo[inline]{Бэкпроп через батчнорм, смысл батчнорма}

\begin{problem}{(инициализация весов)}
    \begin{enumerate}
        \item Маша использует для активации симметричную функцию. Для инициализации весов она хочет использовать распределение 
        
        $$
        w_i \sim U \left[ - \frac{1}{\sqrt{n_{in}}};  \frac{1}{\sqrt{n_{in}}}  \right].
        $$
        
        Покажите, что это будет приводить к затуханию дисперсии при переходе от одного слоя к другому. 
        
        \item Какими нужно взять параметры равномерного распределения, чтобы дисперсия не затухала? 
        
        \item Маша хочет инициализировать веса из нормального распределения. Какими нужно взять параметры, чтобы дисперсия не затухала? 
        
        \item Несимметричный случай
    \end{enumerate}
\end{problem}

\begin{problem}{(ReLU и инициализация весов)}
    Внутри нейрона в качестве функции активации используется ReLU. На вход идёт $10$ признаков. В качестве инициализации для весов используется нормальное распределение, $N(0,1)$. С какой вероятностью нейрон будет выдавать на выход нулевое наблюдение, если 
    
    Предположения на входы? Какое распределение и с какими параметрами надо использовать, чтобы этого не произошло? Сюда же про инициализацию Хе.
\end{problem}


\todo[inline]{задача про инициализацию от Воронцова}




\todo[inline]{родить задачу из статьи dropout vs batchnorm}