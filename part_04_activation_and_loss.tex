% !TEX root = main.tex

%%%-------------------------------------------
\section*{Активация и потери} 

\epigraph{Желание - Ржавый - Семнадцать - Рассвет - Печь - Девять - Добросердечный - Возвращение на Родину - Один - Грузовой вагон.}{\textit{Код активации Зимнего Солдата}}


%%%-------------------------------------------
\begin{problem}{(про сигмоиду)}
Любую "s"-образную функцию называют сигмоидой. Наиболее сильно прославилась под таким названием функция $f(t) = \frac{e^t}{1 + e^t}.$ Слава о ней добралась до Маши и теперь она хочет немного поисследовать её свойства. 

\begin{enumerate}
	\item Выпишите формулы для forward pass и backward pass через слой с сигмоидой. 
	\item Какое максимальное значение принимает производная сигмоиды? Объясните как это способствует затуханию градиента и параличу нейронной сети?  
	\end{enumerate}
\end{problem} 


%%%-------------------------------------------
\begin{problem}{(про тангенс)}
	Функция $f(t) = \tanh(t) = \frac{2}{1 + e^{-2t}} - 1$ называется гиперболическим тангенсом.
	\begin{enumerate}
		\item Что происходит при $t \to +\infty$? А при $t \to -\infty$?
		\item Как связаны между собой $f(t)$ и  $f'(t)$?
		\item Выпишите формулы для forward pass и backward pass через слой с тангенсом. 
		\item Правда ли, что тангенс способствует затуханию градиента и параличу нейронной сети? Какое максимальное значение принимает производная тангенса? 
		\item \todo[inline]{пункт про то, почему часто функцию юзают в RNN}
	\end{enumerate}
\end{problem} 


%%%-------------------------------------------
\begin{problem}{(про ReLU)}
	Функция $f(t) = ReLU(t) = \max(t, 0)$ называется ReLU.
	\begin{enumerate}
        \item 
	\end{enumerate}
\end{problem} 


\todo[inline]{Задача про ReLU и сигмоиду (Николенко) }
\todo[inline]{Задача про паралич сигмоиды и ReLU}



%%%-------------------------------------------
\begin{problem}{(температура генерации)}
	Иногда в функцию $\softmax$ добавляют дополнительный параметр $T$, который называют температурой. Тогда она приобретает вид 
	
	$$ 
	f(z) =  \frac{e^{\tfrac{z_i}{T}}}{ \sum_{k=1}^K e^{\tfrac{z_k}{T}}}
	$$

	Обычно это делается, когда с помощью нейросетки нужно сгенерировать какой-нибудь новый объект.  Пусть у нас есть три класса. Наша нейросеть выдала на последнем слое числа $1,2,5$. 

	\begin{enumerate}
		\item  Какое итоговое распределение вероятностей мы получим, если $T = 10$? 
		\item  А если $T = 1$? 
		\item  А если $T = 0.1$? 
		\item  Какое распределение получится при $T \to 0$? 
		\item  А при $T \to \infty$? 
		\item  Предположим, что объектов на порядок больше. Например, это реплики, которые Алиса может сказать вам в ответ на какую-то фразу.  Понятное дело, что вашей фразе будет релевантно какое-то подмножество ответов. Какое значение температуры сэмплирования $T$ смогут сделать реплики Алисы непредсказуемыми? А какие сделают их однотипными? 
	\end{enumerate}
\end{problem}

