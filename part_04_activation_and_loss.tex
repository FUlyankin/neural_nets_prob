% !TEX root = main.tex

%%%-------------------------------------------
\section{Активация и потери} 

\epigraph{Мудрая цитата про активацию}{Автор цитаты}

%%%-------------------------------------------
\begin{problem}{(про сигмоиду)}
	Функция $f(t) = \frac{e^t}{1 + e^t}$ называется сигмоидой. Вообще говоря сигмоидой называют любую "s"-образную функцию. 
		\begin{enumerate}
		\item Что происходит при $t \to +\infty$? А при $t \to -\infty$?
		\item Как связаны между собой $f(t)$ и  $f(-t)$?
		\item Как связаны между собой $f'(t)$ и  $f'(-t)$?
		\item Как связаны между собой $f(t)$ и $f'(t)$? 
		\item Найдите $f(0)$, $f'(0)$ и $\ln f(0)$.
		\item Найдите обратную функцию $f^{-1}(t)$
		\item Как связаны между собой $\frac{d\ln f(t)}{dt}$ и $f(-t)$?
		\item Постройте графики функций $f(t)$ и $f'(t)$.
		\item Разложите $h(\beta_1, \beta_2)=\ln f(y_i(\beta_1 + \beta_2 x_i))$ в ряд Тейлора до второго порядка в окрестности точки $\beta_1=0$, $\beta_2=0$.
		\item Выпишите формулы для forward pass и backward pass через слой с сигмоидой. 
		\item Правда ли, что сигмоида способствует затуханию градиента и параличу нейронной сети? Какое максимальное значение принимает её производная? 
	\end{enumerate}
\end{problem} 

%%%-------------------------------------------
\begin{problem}{(про $\logloss$)}
	У Маши три наблюдения, первое наблюдение --- кит, остальные --- муравьи. Киты кодируются $y_i = 1$, муравьи --- $y_i = 0$.  В качестве регрессоров Маша берёт номера наблюдений $x_i = i$. После этого Маша оценивает логистическую регрессию с константой.
	
	\begin{enumerate}
		\item Выпишите эмпирическую функцию риска, которую минимизирует Маша;
		
		\item При каких оценках коэффициентов логистической регрессии эта функция достигает своего минимума?
	\end{enumerate}
\end{problem}

%%%-------------------------------------------
\begin{problem}{(про $\softmax$)}
	Маша чуть внимательнее присмотрелась к своему третьему наблюдению и поняла, что это не кит, а бобёр. Теперь ей нужно решать задачу классификации на три класса. Она решил использовать для этого нейросеть с softmax-слоем на выходе. Предположим, что сетка обучилась и на двух новых наблюдениях, перед самым $\softmax$-слоем она выплюнула $1, 2, 5$ и $2, 5, 1$.
	
	\begin{enumerate}
		\item Чему равны вероятности получить кита, муравья и бобра для обеих ситуаций? 
		
		\item Пусть первым был кит, а вторым бобёр.  Чему будет равна $\logloss$-ошибка? 
		
		\item Пусть у Маши есть два класса. Она хочет выучить нейросеть. Она может учить нейронку с одним выходом и сигмоидой в качестве функции активации либо нейронку с двумя выходами и $\softmax$ в качестве функции активации. Как выходы этих двух нейронок взаимосвязаны между собой? 
		
		\todo[inline]{Тут сделать пункт про то почему софтмакс называется софтмаксом}
	\end{enumerate}
\end{problem}

%%%-------------------------------------------
\begin{problem}{(про тангенс)}
	Функция $f(t) = \tanh(t) = \frac{2}{1 + e^{-2t}} - 1$ называется гиперболическим тангенсом.
	\begin{enumerate}
		\item Что происходит при $t \to +\infty$? А при $t \to -\infty$?
		\item Как связаны между собой $f(t)$ и  $f'(t)$?
		\item Выпишите формулы для forward pass и backward pass через слой с тангенсом. 
		\item Правда ли, что тангенс способствует затуханию градиента и параличу нейронной сети? Какое максимальное значение принимает производная тангенса? 
		\item \todo[inline]{пункт про то, почему часто функцию юзают в RNN}
	\end{enumerate}
\end{problem} 

%%%-------------------------------------------
\begin{problem}{(про ReLU)}
	Функция $f(t) = ReLU(t) = \max(t, 0)$ называется ReLU.
	\begin{enumerate}
        \item 
	\end{enumerate}
\end{problem} 


\todo[inline]{Задача про ReLU и сигмоиду (Николенко) }
\todo[inline]{Задача про паралич сигмоиды и ReLU}


%%%-------------------------------------------
\begin{problem}{(про разные выходы)}
Та, в чьих руках находится лёрнинг, решила немного поэкспериментировать с выходами из своей сетки. 

\begin{enumerate}
	\item  Для начала Маша решила, что хочет решать задачу классификации на два класса и получать на выходе вероятность принадлежности к первому. Что ей надо сделать с последним слоем сетки? 
	\item  Теперь Маша хочет решать задачу классификации на $K$ классов. Что ей делать с последним слоем? 
	\item  Новые вводные! Маша хочет спрогнозировать рейтинг фильма на "Кинопоиске". Он измеряется по шкале от $0$ до $10$ и принимает любое непрерывное значение. Как Маша может приспособить для этого свою нейронку? 
	\item У Маши есть куча новостей. Каждая новость может быть спортивной, политической или экономической. Иногда новость может относится сразу к нескольким категориям. Как Маше собрать нейросетку для решения этой задачи?  Как будет выглядеть при этом функция ошибки? 
	\item  Маша пошла в кафе. А там куча народу. Сейчас она сидит за столиком, попивает ванильный топлёный кортадо и думает о нём, о лёрнинге.  Сейчас мысль такая: как можно спрогнозировать число людей в кафе так, чтобы на выходе сетка всегда прогнозировала целое число. Надо ли как-то при этом менять функцию потерь? 
	
	\textbf{Hint:} вспомните про пуассоновскую регрессию.
	% \item[f)] Пункт с регрессией и весами из денег 
\end{enumerate}
\end{problem}


%%%-------------------------------------------
\begin{problem}{(температура генерации)}
	Иногда в функцию $\softmax$ добавляют дополнительный параметр $T$, который называют температурой. Тогда она приобретает вид 
	
	$$ 
	f(z) =  \frac{e^{\tfrac{z_i}{T}}}{ \sum_{k=1}^K e^{\tfrac{z_k}{T}}}
	$$

	Обычно это делается, когда с помощью нейросетки нужно сгенерировать какой-нибудь новый объект.  Пусть у нас есть три класса. Наша нейросеть выдала на последнем слое числа $1,2,5$. 

	\begin{enumerate}
		\item  Какое итоговое распределение вероятностей мы получим, если $T = 10$? 
		\item  А если $T = 1$? 
		\item  А если $T = 0.1$? 
		\item  Какое распределение получится при $T \to 0$? 
		\item  А при $T \to \infty$? 
		\item  Предположим, что объектов на порядок больше. Например, это реплики, которые Алиса может сказать вам в ответ на какую-то фразу.  Понятное дело, что вашей фразе будет релевантно какое-то подмножество ответов. Какое значение температуры сэмплирования $T$ смогут сделать реплики Алисы непредсказуемыми? А какие сделают их однотипными? 
	\end{enumerate}
\end{problem}

