%!TEX TS-program = xelatex
\documentclass[12pt, a4paper, oneside]{article}

\input{preamble.tex}


% \title{Тятя! Тятя! Наши сети притащили мертвеца!}
\title{Тятя! Тятя! Нейросети заменили продавца!}
\date{ }
\author{Ульянкин Филипп}

\begin{document}

% Если переключить в false, все sol исчезнут из pdf
\toggletrue{lecture}
%\togglefalse{lecture}

\maketitle
	
\begin{abstract}
    В этой виньетке собрана коллекция ручных задачек про нейросетки на пару томных вечеров. Вместе с Машей можно попробовать по маленьким шажкам с ручкой и бумажкой раскрыть у себя в теле несколько чакр и немного глубже понять модели глубокого обучения\footnote{Ахахах глубже глубокого, ахахах}.
\end{abstract}

\section*{Вместо введения}
    Однажды Маша услышала про какой-то Машин лёрнинг. Она сразу же смекнула, что именно она та самая Маша, кому этот лёрнинг должен принадлежать. Ещё она смекнула, что если хочет владеть лёрнингом по праву, ни одна живая душа не должна сомневаться в том, что она шарит. Поэтому она постоянно изучает что-то новое. 
    
    Её друг Миша захотел стать адептом Машиного лёрнинга, и спросил её о том, как можно за вечер зашарить алгоритм обратного распространения ошибки. Тогда Маша открыла свою первую книгу по глубокому обучению и прочитала в ней: 
    
    \begin{quote}
    Благодаря символическому дифференцированию вам никогда не придется заниматься реализацией агоритма обратного распространения вручную. Поэтому не будем тратить время на его формулировку\footnote{Франсуа Шолле, Глубокое обучение на Python, стр. 77}.
    \end{quote} 
    
    Маше такая логика показалась странной. Поэтому она взяла книгу с более глубокой математикой. Там она прочитала, что: 
    
    \begin{quote}
    \todo[inline]{Николенко} 
    \end{quote} 
    
    Тогда Маша взяла Библию глубокого обучения\footnote{Goodfellow I., Bengio Y., Courville A. Deep learning. – MIT press, 2016.} и поняла, что по ней за один вечер точно не разберёшься. Слишком серьёзно всё написано. Для вечерних разборок нужно что-то более инфантильное. 
    
    У Маши оставался один выход: поскрести по лёрнингу и собрать инфантильную коллекцию ручных задачек, прорешивая которую новые адепты Машиного лёрнинга могли бы открывать у себя во чакру за чакрой. Так и появилась эта виньетка.  
	
\tableofcontents

\newpage 

\input{part_01_just_function.tex}

\input{part_02_gradient.tex}

\input{part_03_backprop.tex}

\input{part_04_activation_and_loss.tex}

\input{part_05_regularization.tex}


\section{Всего лишь кубики LEGO} 

\epigraph{Какая-нибудь цитата про LEGO}{автор цитаты}

\subsection{Свёртка} 

%% Свёрточные сетки 
\begin{problem}{(Картинка)}
На вход в нейронную сетку идёт изображение размера $28 \times 28$. Маша вытягивает эту картинку в вектор. Он состоит из значений каждого пикселя. 

Дальше в сетке идёт полносвязный слой из $1000$ нейронов. После полносвязный слой, который осуществляет классификацию изображения на $10$ классов. Сколько параметров нужно оценить? 
\end{problem}
\begin{sol}
У нас $28^2 = 784$ входа. Весов между входным и полносвязным слоями будет 

\[ (784 + 1)\cdot 1000 = 785000.\] 

Единица отвечает за константу для каждого из $1000$ нейронов. Между полносвязным и итоговым слоем

\[(1000 + 1) \cdot 10 = 10010. \]
\end{sol} 

\todo[inline]{Задача сделать свёртку своими руками}

\todo[inline]{Задача придумать ядро для классификации крестиков и ноликов}

Придумать ядро которое подсчитывает число слешей на картинке

\todo[inline]{Список ядер с вопросами что они делают с картинкой}

\begin{problem}{(Свёрточный и полносвязный слои)}
Записать свёрточный слой в виде полносвязного. Рисунок + матрица. 
\end{problem}


% https://arxiv.org/pdf/1603.07285.pdf

\todo[inline]{Упражнения про падинги и про то какой рецептиф филд}


\subsection{Рекурентные сетки} 


\todo[inline]{простые задачи про RNN и LSTM}

\todo[inline]{простые задачи про RNN и LSTM}

\todo[inline]{какие-нибудь упражнения про w2v}

\todo[inline]{упражнение про разные модные виды ячеек типа резнетов и тп}



\section{Итоговый тест в стиле Носко}


\begin{enumerate} 
\item Вопрос про батчнормализацию первым слоем вместо нормализации в предобработке. 
\end{enumerate}


\end{document}
