% !TEX root = main.tex

%%%-------------------------------------------
\section{Всего лишь кубики LEGO} 

\subsection{Функции активации}

\epigraph{Желание - Ржавый - Семнадцать - Рассвет - Печь - Девять - Добросердечный - Возвращение на Родину - Один - Грузовой вагон.}{\textit{Код активации Зимнего Солдата}}


%%%-------------------------------------------
\begin{problem}{(про сигмоиду)}
Любую "s"-образную функцию называют сигмоидой. Наиболее сильно прославилась под таким названием функция $f(t) = \frac{e^t}{1 + e^t}.$ Слава о ней добралась до Маши и теперь она хочет немного поисследовать её свойства. 

\begin{enumerate}
	\item Выпишите формулы для forward pass и backward pass через слой с сигмоидой. 
	\item Какое максимальное значение принимает производная сигмоиды? Объясните как это способствует затуханию градиента и параличу нейронной сети?  
	\end{enumerate}
\end{problem} 


%%%-------------------------------------------
\begin{problem}{(про тангенс)}
	Функция $f(t) = \tanh(t) = \frac{2}{1 + e^{-2t}} - 1$ называется гиперболическим тангенсом.
	\begin{enumerate}
		\item Что происходит при $t \to +\infty$? А при $t \to -\infty$?
		\item Как связаны между собой $f(t)$ и  $f'(t)$?
		\item Выпишите формулы для forward pass и backward pass через слой с тангенсом. 
		\item Правда ли, что тангенс способствует затуханию градиента и параличу нейронной сети? Какое максимальное значение принимает производная тангенса? 
		\item \todo[inline]{пункт про то, почему часто функцию юзают в RNN}
	\end{enumerate}
\end{problem} 


%%%-------------------------------------------
\begin{problem}{(про ReLU)}
	Функция $f(t) = ReLU(t) = \max(t, 0)$ называется ReLU.
	\begin{enumerate}
        \item 
	\end{enumerate}
\end{problem} 


\todo[inline]{Задача про ReLU и сигмоиду (Николенко) }
\todo[inline]{Задача про паралич сигмоиды и ReLU}


\todo[inline]{Parametric Rectifier (PReLU) Выписать уравнения для бэкпропа по параметру $\alpha$} 



%%%-------------------------------------------
\begin{problem}{(температура генерации)}
	Иногда в функцию $\softmax$ добавляют дополнительный параметр $T$, который называют температурой. Тогда она приобретает вид 
	
	$$ 
	f(z) =  \frac{e^{\tfrac{z_i}{T}}}{ \sum_{k=1}^K e^{\tfrac{z_k}{T}}}
	$$

	Обычно это делается, когда с помощью нейросетки нужно сгенерировать какой-нибудь новый объект.  Пусть у нас есть три класса. Наша нейросеть выдала на последнем слое числа $1,2,5$. 

	\begin{enumerate}
		\item  Какое итоговое распределение вероятностей мы получим, если $T = 10$? 
		\item  А если $T = 1$? 
		\item  А если $T = 0.1$? 
		\item  Какое распределение получится при $T \to 0$? 
		\item  А при $T \to \infty$? 
		\item  Предположим, что объектов на порядок больше. Например, это реплики, которые Алиса может сказать вам в ответ на какую-то фразу.  Понятное дело, что вашей фразе будет релевантно какое-то подмножество ответов. Какое значение температуры сэмплирования $T$ смогут сделать реплики Алисы непредсказуемыми? А какие сделают их однотипными? 
	\end{enumerate}
\end{problem}


%%%-------------------------------------------
\subsection{Регуляризация}

\epigraph{Цитата про переобучение}{Автор цитаты}


\begin{problem}{(Маша и покемоны)}
 Маша измерила вес трёх покемонов,  $y_1=6$, $y_2=6$, $y_3=10$.  Она хочет спрогнозировать вес следующего покемона. Модель для веса покемонов у Маши очень простая, $y_i = \beta + \varepsilon_i$, поэтому прогнозирует Маша по формуле $\hat y_i = \hat \beta$.
	
	Для оценки параметра $\beta$ Маша использует следующую целевую функцию:
	
	$$
	\sum (y_i - \hat \beta)^2 + \lambda \cdot \hat \beta^2
	$$
	
	\begin{enumerate}
		\item[a)] Найдите оптимальное $\hat \beta$ при $\lambda =0$.
		\item[б)] Найдите оптимальное $\hat \beta$ при произвольном $\lambda$. Правда ли, что чем больше $\lambda$, тем меньше $\beta$? 
		\item[в)] Подберите оптимальное $\lambda$ с помощью кросс-валидации leave one out («выкинь одного»). При такой валидации на первом шаге мы оцениваем модель на всей выборке без первого наблюдения, а на первом тестируем её. На втором шаге мы оцениваем модель на всей выборке без второго наблюдения, а на втором тестируем её. И так далее $n$ раз. Каждое наблюдение является отдельным фолдом.
		\item[г)] Найдите оптимальное $\hat \beta$ при $\lambda_{CV}$.
	\end{enumerate}
\end{problem}


\begin{problem}{(а вот и моя остановочка)}
    \todo[inline]{Сделать задачу по связи ранней остановки и регуляризатора. Как в книжке про диплернинг}
\end{problem}


\begin{problem}{(дропаут)}
Маша собирается обучить нейронную сеть для решения задачи регрессии, На вход в неё идёт $12$ переменных, в сетке есть $3$ скрытых слоя. В пером слое $300$ нейронов, во втором $200$, в третьем $100$. 

	\begin{enumerate}
		\item[a)] Сколько параметров предстоит оценить Маше?  Сколько наблюдений вы бы на её месте использовали? 
		\item[b)] Пусть в каждом слое была отключена половина нейронов. Сколько коэффициентов необходимо оценить?
		\item[c)] Предположим, что Маша решила после первого слоя добавить в свою сетку Dropout c вероятностью $p$.  Какова вероятность того, что отключится весь слой? 
		\item[d)] Маша добавила Dropout c вероятностью $p$. после каждого слоя. Какова вероятность того, что один из слоёв отключится и сетка не сможет учиться? 
		\item[e)] Пусть случайная величина $N$ --- это число включённых нейронов. Найдите её математическое ожидание и дисперсию. Если Маша хочет проредить сетку на четверть, какое значение $p$ она должна поставить? 
		\item[f)] Пусть случайная величина $P$ --- это число параметров в нейросети, которое необходимо оценить. Найдите её математическое ожидание и дисперсию. Почему найденное вами математическое ожидание выглядит очень логично? Что оно вам напоминает? Обратите внимание на то, что смерть одного из параметров легко может привести к смерти другого.
	\end{enumerate}
\end{problem}

\todo[inline]{Добавить вопросиков про дропконнект}

\todo[inline]{Бэкпроп через дропаут}


%%%-------------------------------------------
\subsection{Нормализация по батчам}

\epigraph{Чашка хорошего чая восстановит мою нормальность.}{\textit{Артур из «Автостопом по галактике»}}

\todo[inline]{Бэкпроп через батчнорм, смысл батчнорма}


\todo[inline]{родить задачу из статьи dropout vs batchnorm}


%%%-------------------------------------------
\subsection{Инициализация}

\epigraph{цитата об этом}{\textit{автор}}


\begin{problem}{(инициализация весов)}
    \begin{enumerate}
        \item Маша использует для активации симметричную функцию. Для инициализации весов она хочет использовать распределение 
        
        $$
        w_i \sim U \left[ - \frac{1}{\sqrt{n_{in}}};  \frac{1}{\sqrt{n_{in}}}  \right].
        $$
        
        Покажите, что это будет приводить к затуханию дисперсии при переходе от одного слоя к другому. 
        
        \item Какими нужно взять параметры равномерного распределения, чтобы дисперсия не затухала? 
        
        \item Маша хочет инициализировать веса из нормального распределения. Какими нужно взять параметры, чтобы дисперсия не затухала? 
        
        \item Несимметричный случай
    \end{enumerate}
\end{problem}

\begin{problem}{(ReLU и инициализация весов)}
    Внутри нейрона в качестве функции активации используется ReLU. На вход идёт $10$ признаков. В качестве инициализации для весов используется нормальное распределение, $N(0,1)$. С какой вероятностью нейрон будет выдавать на выход нулевое наблюдение, если 
    
    Предположения на входы? Какое распределение и с какими параметрами надо использовать, чтобы этого не произошло? Сюда же про инициализацию Хе.
\end{problem}


\todo[inline]{задача про инициализацию от Воронцова}


\subsection{Стрельба по ногам}

\begin{problem}{(Проблемы с архитектурой)}
Миша принёс Маше несколько разных архитектур. Они выглядят довольно странно. Помогите Маше разобраться, что именно Миша сделал неправильно. 

\begin{enumerate} 

\item Решается задача регрессии, предсказываются цены на недвижимость. 

% model = Sequential()
% model.add(InputLayer([39]))
% model.add(BatchNormalization())
% model.add(Dense(128, kernel_initializer=keras.initializers.zeros()))
% model.add(Dense(128, kernel_initializer=keras.initializers.zeros()))
% model.add(Dense(1))

% model.compile(optimizer='sgd', loss='mean_squared_error')

\item Решается задача классификация картинок на $10$ классов. Исходный размер картинок $28 \times 28$.

% model = keras.models.Sequential()
% model.add(L.InputLayer([28, 28, 1]))
% model.add(L.Conv2D(filters=512, kernel_size=(3, 3)))
% model.add(L.Activation('relu'))
% model.add(L.MaxPool2D(pool_size=(2, 2)))
% model.add(L.Flatten())
% model.add(L.Dense(100))
% model.add(L.Activation('softmax'))
% model.add(L.Dropout(0.1))
% model.add(L.Dense(10))
% model.add(L.Activation('softmax'))
% model.add(L.Dropout(0.1))

% model.compile(optimizer='rmsprop', loss='mean_squared_error')

\item Решается задача классификация картинок на $10$ классов. Исходный размер картинок $100 \times 100$.

% model = keras.models.Sequential()
% model.add(L.InputLayer([100, 100, 3]))

% for filters in [32, 64, 128, 256]:
%     model.add(L.Conv2D(filters, kernel_size=(5, 5)))
%     model.add(L.Conv2D(filters, kernel_size=(1, 1)))
%     model.add(L.MaxPooling2D(pool_size=(3, 3)))
%     model.add(L.Activation('relu'))
%     model.add(L.BatchNormalization())

% model.add(L.Flatten())

% model.add(L.Dense(100, activation='relu'))
% model.add(L.Dropout(0.5))
% model.add(L.Dense(10, activation='softmax'))

% model.compile(optimizer='adam', loss='sparse_categorical_accuracy')


\end{enumerate} 


\end{problem}





% \begin{enumerate} 
% \item Вопрос про батчнормализацию первым слоем вместо нормализации в предобработке, задачи про последовательности слоёв и поиск ошибок 
% \end{enumerate}





