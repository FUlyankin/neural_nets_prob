% !TEX root = main.tex

%%%-------------------------------------------
\section{Матричное дифФфФфФфириенцирование\footnote{Часть задач взята из \href{https://github.com/bdemeshev/mlearn\_pro/blob/master/mlearn\_pro.pdf}{прототипа задачника по ML  Бориса Демешева}, часть из \href{https://github.com/esokolov/ml-course-msu}{конспектов по ML Жени Соколова} } }

\epigraph{ $\bigg(\text{ \includegraphics[scale=0.08]{tree1.png}} \bigg)^T = $  \includegraphics[scale=0.08]{tree2.png} }{<<Джек и бобовый стебель>> (1890)}


%%%-------------------------------------------
\begin{problem}{}	
	Найдите следующие производные:
	\begin{enumerate}	
		
		\item $f(x) = a^T x$, где  $a$  и $x$ векторы размера $1 \times n$ 
		
		\item $f(x) = x^T A x$, где $x$ вектор размера $1 \times n$, $A$ матрица размера $n \times n$
		
		\item $f(x) = \ln(x^T A x)$, где $x$ вектор размера $1 \times n$, $A$ матрица размера $n \times n$
		
		\item $f(x) = a^TXAXa$, где $x$ вектор размера $1 \times n$, $A$ матрица размера $n \times n$
		
		\item $f(x) = x x^T x$,  где $x$ вектор размера $1 \times n$
		
		\item $f(X) = X^{-1}$, где матрица $X$ размера $n \times n$
		
		\item  $f(X) = \det X$, где матрица $X$ размера $n \times n$
	\end{enumerate}
\end{problem} 


%%%-------------------------------------------
\begin{problem}{}
	Найдите следующие производные:
	\begin{enumerate}
		\item $f(X) = \tr(AXB)$, где матрица $A$ размера $p \times m$, матрица $B$ размера $n \times p$, матрица $X$ размера $m \times n$. 
		
		\item $f(X) = \tr(AX^TX)$, где матрица $A$ размера $n \times n$, матрица $X$ размера $m \times n$. 
		
		\item $f(X) = \ln \det X$
		
		\item $f(X) = \ln AX^{-1}B$  % убрать эту задачу 
		
		\item $f(X) = \tr(AX^TXBX^{-T})$
		
		\item $f(X) = \ln \det(X^TAX)$
		
		\item $f(x) = x^TAb$, где матрица $A$ размера $n \times n$, вектора $x$ и $b$ размера $n \times 1$. 
		
		\item $f(A) = x^TAb$. 
	\end{enumerate}
\end{problem}

\begin{problem}{}
	Рассмотрим задачу линейной регресии
	\[
	Q(w) = (y - Xw)^T(y - Xw) \to \min_{w}.
	\]
	
	\begin{enumerate}
		\item Найдите $dQ(w)$, выведите формулу для оптимального $w$.
		\item Как выглядит шаг градиентного спуска в матричном виде?
		\item Найдите $d^2Q(w)$. Убедитесь, что мы действительно в точке минимума. 
	\end{enumerate}
\end{problem}

\begin{problem}{}
	В случае Ridge-регрессии минимизируется функция
	\[
	Q(w) = (y - Xw)^T(y - Xw) + \lambda w^T w,
	\]
	где $\lambda$ — положительный параметр, штрафующий функцию за слишком большие значения $w$.
	
	\begin{enumerate}
		\item Найдите $dQ(w)$, выведите формулу для оптимального $w$.
		\item Как выглядит шаг градиентного спуска в матричном виде?
		\item Найдите $d^2Q(w)$. Убедитесь, что мы действительно в точке минимума. 
	\end{enumerate}
	
	В случае Lasso-регрессии мы имеем дело с функцией
	\[
	Q(w) = (y - Xw)^T(y - Xw) + \lambda |w|,
	\]
	
	\begin{enumerate}
		\item Найдите $dQ(w)$, выведите формулу для оптимального $w$.
		\item Как выглядит шаг градиентного спуска в матричном виде?
	\end{enumerate}
\end{problem}

\begin{problem}{}
	Пусть $x_i$ — вектор-столбец $k\times 1$, $y_i$ — скаляр, равный $+1$ или $-1$, $w$ — вектор-столбец размера $k\times 1$. Рассмотрим логистическую функцию потерь для линейной модели
	\[
	Q(w) = \sum_{i=1}^n \ln (1 + \exp(-y_ix_i^Tw)) + \lambda w^T w
	\]
	
	\begin{enumerate}
		\item Найдите $dQ$;
		\item Найдите вектор-столбец $\nabla Q$.
		\item Как для этой функции потерь выглядит шаг градиентного спуска в матричном виде? 
	\end{enumerate}
\end{problem}


\begin{problem}{}
	Упражняемся в матричном методе максимального правдоподобия.  Допустим, что векторы $X_1, \ldots, X_m$ выбраны из многомерного нормального распределения с неизвестными вектором средних $\mu$ и ковариационной матрицей $\Sigma$. В этом задании нужно найти оценки максимального правдоподобия для $\hat \mu$ и $\hat \Sigma$.  Обратите внимание, что выборкой здесь будет не $x_1, \ldots, x_m$, а 
	\begin{equation*}
	\begin{pmatrix}
	x_{11}, \ldots, x_{m1} \\
	\ldots  \\ 
	x_{1n}, \ldots, x_{mn}
	\end{pmatrix}
	\end{equation*}
\end{problem}


\begin{problem}{}
	Найдите симметричную матрицу $X$ наиболее близкую к матрице $A$ по норме Фробениуса, $\sum_{i,j} (x_{ij} - a_{ij})^2$. Тут мы просто из каждого элемента вычитаем каждый и смотрим на сумму квадратов таких разностей. 
	
	То есть решите задачку условной матричной минимизации 
	
	\begin{equation*}
	\begin{cases}
	& ||X - A||^2 \to \min_{A}  \\
	& X^T = X
	\end{cases}
	\end{equation*}
	
	\textbf{Hint:} Надо будет выписать Лагранджиан.  А ещё пригодится тот факт, что $\sum_{i,j} (x_{ij} - a_{ij})^2 = ||X-A||^2 =  \tr((X-A)^T (X-A))$. То, что это так мы доказали на семинаре :) Вспоминайте! 
\end{problem}

\todo[inline]{Придумать ещё какую-нибудь задачу оптимизации} 

