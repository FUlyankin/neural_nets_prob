% !TEX root = main.tex

%%%-------------------------------------------
\section{Матричное дифФфФфФференцирование}

% \section{Матричное дифФфФфФференцирование\footnote{Часть задач взята из \href{https://github.com/bdemeshev/mlearn\_pro/blob/master/mlearn\_pro.pdf}{прототипа задачника по ML  Бориса Демешева}, часть из \href{https://github.com/esokolov/ml-course-hse}{конспектов по ML Жени Соколова}}}

\epigraph{ $\bigg(\text{ \includegraphics[scale=0.08]{tree1.png}} \bigg)^T = $  \includegraphics[scale=0.08]{tree2.png} }{<<Джек и бобовый стебель>> (1890)}

%%%-------------------------------------------
\begin{problem}{}	
	Найдите следующие производные:
	\begin{enumerate}	
	
	    \item $f(x) = x^2$, где $x$ скаляр
		
		\item $f(x) = a^T x$, где  $a$  и $x$ векторы размера $1 \times n$ 
		
		\item $f(x) = x^T A x$, где $x$ вектор размера $1 \times n$, $A$ матрица размера $n \times n$
		
 		\item $f(x) = \ln(x^T A x)$, где $x$ вектор размера $1 \times n$, $A$ матрица размера $n \times n$
		
		\item $f(x) = a^TXAXa$, где $x$ вектор размера $1 \times n$, $A$ матрица размера $n \times n$
		
 		\item $f(x) = x x^T x$,  где $x$ вектор размера $1 \times n$
	\end{enumerate}
\end{problem} 
\begin{sol}
Зачем нам нужно научиться искать матричные производные? \indef{Машинное обучение --- это сплошная оптимизация.} В нём мы постоянно вынуждены искать минимум какой-нибудь штрафной функции. Матричные производные довольно сильно в этом помогают\footnote{В тексте про матричные производные я опирался на \href{http://www.machinelearning.ru/wiki/images/5/50/MOMO17_Seminar2.pdf}{этот конспект.} Для того, чтобы познакомиться с темой более широко, имеет смысл прочитать его.}. 

Когда мы работаем с одномерными функциями, для поиска любых производных нам хватает небольшой таблицы со стандартными случаями и пары правил. Для случая матриц все эти правила можно обобщить, а таблицы дополнить специфическими функциями вроде определителя. \indef{Удобнее всего оказывается работать в терминах «дифференциала» --- с ним можно не задумываться о промежуточных размерностях, а просто применять стандартные правила.}

Мы будем работать в этом конспекте со скалярами, векторами и матрицами. Нас будет интересовать, что именно мы дифференцируем, по чему мы дифференцируем и что получается в итоге.  

Строчными буквами мы будем обозначать векторы-столбцы и константы. Заглавными буквами мы будем обозначать матрицы. Производная столбца --- это столбец. Производная по столбцу --- это столбец. 

\[
x = \begin{pmatrix}x_1 \\ \ldots \\ x_n \end{pmatrix} \qquad X = \begin{pmatrix}x_{11} & \ldots & x_{1n} \\ \vdots & \ddots & \vdots \\ x_{n1} & \ldots & x_{nn}  \end{pmatrix}.
\]

Мы рассмотрим постепенно много разных входов и выходов,  и получим таблицу из канонических случаев. По строчкам будем откладывать то, откуда бьёт функция, то есть входы. По столбцам будем откладывать то, куда бьёт функция, то есть выходы. Для ситуаций обозначенных прочерками обобщения получить не выйдет. 

\begin{center} 
    \begin{tabular}{|c|c|c|c|}
    \hline
        & скаляр & вектор & матрица \\
    \hline 
    скаляр & $f'(x) \dx{x}$  &  $\mathfrak{J} * \dx{x}$  & ---          \\
    \hline
    вектор & $\nabla f^T \dx{x}$    &   $\mathfrak{J} \dx{x}$     &  ---        \\
    \hline
    матрица & $\tr(\nabla f^T \dx{X})$    &    ---    & ---        \\
    \hline
    \end{tabular}
\end{center} 

Символом $\nabla^T f$ обозначается градиент (вектор из производных). Символом $\mathfrak{J}$ обозначена матрица Якоби. Символом $H$ мы будем обозначать матрицу Гессе из вторых производных. 

\begin{enumerate} 
\item Начнём с уже известного нам случая: функция бьёт из скаляров в скаляры

\[
f(x) : \RR \to \RR.
\]

Примером такой функции может быть $f(x) = x^2$. Мы знаем, что по таблице производных $f'(x) = 2x$. Также мы знаем, что \indef{дифференциал} --- это линейная часть приращения функции, а \indef{производная} --- это предел отношения приращения функции к приращению аргумента при приращении аргумента стремящемся к нулю.

Грубо говоря,  дифференциал помогает представить приращение функции в линейном виде

\[
\dx{f(x)} = f'(x) \dx{x}.
\]

Если мы находимся в какой-то точке $x_0$ и делаем из неё небольшое приращение $\dx{x},$ то наша функция изменится примерно на $\dx{f(x)}$. \indef{Оказывается, что именно в терминах дифференциалов удобно работать с матричными производными.}

Свойства матричных дифференциалов очень похожи на свойства обычных. Надо только не забыть, что мы работаем с матрицами.

\begin{equation*}
    \begin{aligned} 
    & \dx{(XY)} = \dx{X}Y + X\dx{Y}, \quad \dx{X}Y \ne Y\dx{X} \\
    & \dx{(\alpha X + \beta Y)} = \alpha \dx{X} + \beta \dx{Y} \\
    & \dx{(X^T)} = (\dx{X})^T \\
    & \dx{A} = 0, \quad A - \text{матрица из констант}
    \end{aligned} 
\end{equation*}

Чтобы доказать все эти свойства достаточно просто аккуратно расписать их. Кроме этих правил нам понадобится пара трюков по работе со скалярами. Если $s$ --- скаляр размера $1 \times 1$, тогда $s^T = s$ и $\tr(s) = s$, где $\tr$ --- операция взятия следа матрицы. 

С помощью этих преобразований мы будем приводить дифференциалы к каноническому виду и вытаскивать из них производные. 

\item Рассмотрим вторую ситуацию из таблицы, функция бьёт из векторов в скаляры. Это обычная функция от нескольких аргументов

\[
f(x) : \RR^n \to \RR.
\]

Такие производные мы брать также умеем. Если мы хотим найти производную функции $f(x_1, x_2, \ldots, x_n)$, нам надо взять производную по каждому аргументу и записать их все в виде вектора. Такой вектор \indef{называют градиентом} 

\[
\nabla f = \begin{pmatrix} \frac{\partial f}{\partial x_1} \\ \frac{\partial f}{\partial x_2} \\ \ldots  \\ \frac{\partial f}{\partial x_n} \end{pmatrix}
\]

Если умножить градиент на вектор приращений, у нас получится дифференциал

\[
\dx{f(x)} = \nabla f^T \dx{x} = \begin{pmatrix} \frac{\partial f}{\partial x_1} & \frac{\partial f}{\partial x_2} & \ldots  & \frac{\partial f}{\partial x_n} \end{pmatrix} \begin{pmatrix} \dx{x_1} \\ \dx{x_2} \\ \ldots  \\ \dx{x_n} \end{pmatrix} = \frac{\partial f}{\partial x_1} \cdot \dx{x_1} + \frac{\partial f}{\partial x_2} \cdot \dx{x_2} + \ldots +\frac{\partial f}{\partial x_n} \cdot \dx{x_n}.
\]

При маленьком изменении $x_i$ на $\dx{x_i}$ функция будет при прочих равных меняться пропорционально соответствующей частной производной. Посмотрим на конкретный пример, \indef{скалярное произведение}. Можно расписать умножение одного вектора на другой в виде привычной нам формулы

\begin{equation*}
\underset{[1 \times 1]}{f(x)} = \underset{[1 \times n]}{a^T} \cdot \underset{[n \times 1]}{x} = \begin{pmatrix} a_1 & a_2 & \ldots &a_n \end{pmatrix} \cdot \begin{pmatrix} x_1 \\ x_2 \\ \ldots  \\ x_n \end{pmatrix} = a_1 \cdot x_1 + a_2 \cdot x_2 + \ldots + a_n \cdot x_n.
\end{equation*} 

Из неё чётко видно, что $\frac{\partial f}{\partial x_i} = a_i$. Увидев это мы можем выписать градиент функции 

\[
\nabla f = \begin{pmatrix} \frac{\partial f}{\partial x_1} \\ \frac{\partial f}{\partial x_2} \\ \ldots  \\ \frac{\partial f}{\partial x_n} \end{pmatrix} = \begin{pmatrix} a_1 \\ a_2 \\ \ldots  \\ a_n \end{pmatrix} = a,
\]

теперь можно записать дифференциал

\[
\dx{f} = a^T \dx{x} = \frac{\partial f}{\partial x_1} \cdot \dx{x_1} + \frac{\partial f}{\partial x_2} \cdot \dx{x_2} + \ldots +\frac{\partial f}{\partial x_n} \cdot \dx{x_n} = a_1 \cdot \dx{x_1} + a_2 \cdot \dx{x_2} + \ldots + a_n \cdot \dx{x_n}.
\]

В то же самое время можно было бы просто воспользоваться правилами нахождения матричных дифференциалов

\[
\dx{f} =  \dx{a^T x} = a^T \dx{x} = \nabla f^T \dx{x},
\]

откуда $ \nabla f = a$. Производная найдена. При таком подходе нам не надо анализировать каждую частную производную по отдельности. Мы находим одним умелым движением руки сразу же все производные. Давайте немного усложним задачу и увидим его мощь во всей красе.

\item Функция по-прежнему бьёт из векторов в скаляры. Попробуем перемножить все матрицы и расписать её в явном виде по аналогии со скалярным произведением 


\begin{equation*}
\underset{[1 \times 1]}{f(x)} = \underset{[1 \times n]}{x^T} \cdot \underset{[n \times n]}{A} \cdot \underset{[n \times 1]}{x} = \sum_{i = 1}^n \sum_{j=1}^n a_{ij} \cdot x_i \cdot x_j.
\end{equation*} 

Если продолжить в том же духе, мы сможем найти все частные производные, а потом назад вернём их в матрицу. Единственное, что смущает --- \indef{мы делаем что-то неестественное.} Всё было записано в красивом компактном матричном виде, а мы это испортили. А что, если множителей будет больше? Тогда суммы станут совсем громоздкими, и мы легко запутаемся. 

При этом, если воспользоваться тут правилами работы с матричными дифференциалами, мы легко получим красивый результат

\[
\dx{f} =  \dx{x^T A x} =  \dx{(x^T)} A x + x^T \dx{(Ax)} =  \dx{(x^T)} A x + x^T \underset{\dx{A} = 0}{\dx{(A)}} x + x^T A \dx{(x)}.
\]

Заметим, что $\dx{(x^T)} A x$ это скаляр. Мы перемножаем матрицы с размерностями $1 \times n$, $n \times n$ и $n \times 1$. В результате получается размерность $1 \times 1$. Мы можем смело транспонировать скаляр, когда нам это надо. Эта операция никак не повлияет на результат

\[
\dx{f} = \dx{(x^T)} A x + x^T A \dx{(x)} = x^T A^T \dx{x}  + x^T A \dx{x} = x^T(A^T + A) \dx{x}.
\]

Мы нашли матричный дифференциал и свели его к каноничной форме 

\[
\dx{f} = \nabla^T f \dx{x} = x^T(A^T + A) \dx{x}
\]

Получается, что искомая производная $\nabla f = (A + A^T) x$. Обратите внимание, что размерности не нарушены и мы получили столбец из производных, то есть искомый градиент нашей функции $f$. 

По аналогии мы легко можем найти вторую производную. Для этого надо взять производную производной. Функция $g(x) = (A + A^T) x$ бьёт из векторов в вектора

\[
f(X) : \RR^n \to \RR^m.
\]

На самом деле с такой ситуацией мы также знакомились на математическом анализе. Если $n=1$ то у нас есть $m$ функций, каждая из которых применяется к $x$. На выходе получается вектор 

\[
\begin{pmatrix} f_1(x) \\ f_2(x) \\ \ldots  \\ f_m(x). \end{pmatrix}
\]

Если мы хотим найти производную, нужно взять частную производную каждой функции по $x$ и записать в виде вектора. Дифференциал также будет представлять из себя вектор, так как при приращении аргумента на какую-то величину изменяется каждая из функций 

\[
\dx{f(x)} = \begin{pmatrix} \frac{\partial f_1}{\partial x} \\ \frac{\partial f_2}{\partial x} \\ \ldots  \\ \frac{\partial f_m}{\partial x} \end{pmatrix} * \begin{pmatrix} \dx{x} \\ \dx{x} \\ \ldots  \\ \dx{x} \end{pmatrix}  = \begin{pmatrix} \frac{\partial f_1}{\partial x} \dx{x} \\ \frac{\partial f_2}{\partial x} \dx{x} \\ \ldots  \\ \frac{\partial f_m}{\partial x} \dx{x} \end{pmatrix}.
\]

Под символом $*$ имеется в виду поэлементное умножение. Если $n > 1$, то аргументов на вход в такой вектор из функций идёт несколько, на выходе получается матрица 

\[
\begin{pmatrix} f_1(x_1) & f_1(x_2) & \ldots & f_1(x_n) \\ f_2(x_1)  & f_2(x_2) & \ldots & f_2(x_n)  \\ \ldots & \ldots & \ddots & \ldots  \\ f_m(x_1)  & f_m(x_2) & \ldots & f_m(x_n) \end{pmatrix}
\]

Производной такой многомерной функции будет матрица из частных производных каждой функции по каждому аргументу

\[
\begin{pmatrix} \frac{\partial f_1}{\partial x_1} & \frac{\partial f_1}{\partial x_2} & \ldots & \frac{\partial f_1}{\partial x_n} \\ \frac{\partial f_2}{\partial x_1}  & \frac{\partial f_2}{\partial x_2} & \ldots & \frac{\partial f_2}{\partial x_n}  \\ \ldots & \ldots & \ddots & \ldots  \\ \frac{\partial f_m}{\partial x_1}  & \frac{\partial f_m}{\partial x_2} & \ldots & \frac{\partial f_m}{\partial x_n} \end{pmatrix}.
\]

Дифференциал снова будет представлять из себя вектор

\[
\dx{f(x)} =  \begin{pmatrix} \frac{\partial f_1}{\partial x_1} & \frac{\partial f_1}{\partial x_2} & \ldots & \frac{\partial f_1}{\partial x_n} \\ \frac{\partial f_2}{\partial x_1}  & \frac{\partial f_2}{\partial x_2} & \ldots & \frac{\partial f_2}{\partial x_n}  \\ \ldots & \ldots & \ddots & \ldots  \\ \frac{\partial f_m}{\partial x_1}  & \frac{\partial f_m}{\partial x_2} & \ldots & \frac{\partial f_m}{\partial x_n} \end{pmatrix} \cdot  \begin{pmatrix} \dx{x_1} \\ \dx{x_2} \\ \ldots  \\ \dx{x_n} \end{pmatrix} =  \begin{pmatrix} \frac{\partial f_1}{\partial x_1} \dx{x_1} + \frac{\partial f_1}{\partial x_2} \dx{x_2} + \ldots + \frac{\partial f_1}{\partial x_n} \dx{x_n}  \\ \frac{\partial f_2}{\partial x_1} \dx{x_1} +  \frac{\partial f_2}{\partial x_2} \dx{x_2} + \ldots + \frac{\partial f_2}{\partial x_n} \dx{x_n}  \\ \ldots  \\ \frac{\partial f_m}{\partial x_1} \dx{x_1} + \frac{\partial f_m}{\partial x_2} \dx{x_2} + \ldots + \frac{\partial f_m}{\partial x_n} \dx{x_n} \end{pmatrix}.
\]

В обоих ситуациях мы можем записать дифференциал через матричное произведение. Вернёмся к поиску второй производной

\[
\dx{g(x)} = (A + A^T) \dx{x}.
\]

Выходит, что матрица из вторых производных для функции $f(x)$ выглядит как $A + A^T.$ Обратите внимание, что для этой ситуации в каноническом виде нет транспонирования. Когда мы вытаскиваем из записи дифференциала производную, нам не надо его применять.


\item Когда мы хотим найти производную  $f(x) = \ln(x^T A x)$, мы просто можем применить правила взятия производной от сложной функции $f(y) = \ln(y).$ К логарифму на вход идет скаляр, а значит его производная равна $\frac{1}{y}$. Выходит, что 

\[
\dx{f(x)} = \dx{\ln(y)} = \frac{1}{y} \dx{y} = \frac{1}{y} \dx{(x^T A x)} = \frac{1}{x^T A x} \cdot x^T(A^T + A) \dx{x}.
\]

В таких ситуация нужно быть осторожным и следить за тем, что на вход функции идёт скаляр. Если это не так, то ситуация усложняется и мы оказываемся в ситуации, где надо поднапрячься. Чуть ниже мы рассмотрим в качестве примера матричную экспоненту $\exp(X),$ где $X$ --- квадратная матрица. 


\item \label{tr_tr} Движемся к следующей ситуации. Функция бьёт из матриц в скаляры

\[
f(X) : \RR^{n \times k} \to \RR.
\]

В таком случае нам надо найти производную функции по каждому элементу матрицы, то есть дифференциал будет выглядеть как 

\[
\dx{f(X)} = f'_{x_{11}} \dx{x_{11}} + f'_{x_{12}} \dx{x_{12}} + \ldots + f'_{x_{nk}} \dx{x_{nk}}.
\]

Его можно записать в компактном виде через след матрицы как 
\[
\dx{f(X)} = \tr(\nabla f^T \dx{X}),
\] где 

\[
\nabla f = \begin{pmatrix} f'_{x_{11}} & \ldots & f'_{x_{1k}} \\ 
\hdots & \ddots & \hdots \\ 
f'_{x_{n1}} & \ldots & f'_{x_{nk}}
\end{pmatrix}
\]

Вполне естественен вопрос \indef{а почему это можно записать именно так?} Давайте попробуем увидеть этот факт на каком-нибудь простом примере. Пусть у нас есть две матрицы 

\[
A_{[2 \times 3]} = \begin{pmatrix} a_{11} & a_{12} & a_{13} \\ a_{21} & a_{22} & a_{23} \end{pmatrix} \qquad X_{[2 \times 3]} = \begin{pmatrix} x_{11} & x_{12} & x_{13} \\ x_{21} & x_{22} & x_{23} \end{pmatrix}.
\]

Посмотрим на то, как выглядит $\tr(A^T \dx{X})$. Как это не странно, он совпадает с дифференциалом 

\[\tr(A^T \dx{X}) = \tr \left( \begin{pmatrix} a_{11} & a_{21} \\ a_{12} & a_{22} \\ a_{13} &  a_{23} \end{pmatrix} \begin{pmatrix} \dx x_{11} & \dx x_{12} & \dx x_{13} \\ \dx x_{21} & \dx x_{22} & \dx x_{23} \end{pmatrix} \right),
\]

при произведении на выходе получаем матрицу размера $3 \times 3$

\[
\begin{pmatrix} a_{11} \dx x_{11} +  a_{21} \dx x_{21} & a_{11} \dx x_{12} +  a_{21} \dx x_{22} & a_{11} \dx x_{13} +  a_{21} \dx x_{23} \\ a_{12} \dx x_{11} +  a_{22} \dx x_{21} & a_{12} \dx x_{12} +  a_{22} \dx x_{22} & a_{12} \dx x_{13} +  a_{22} \dx x_{23} \\ a_{13} \dx x_{11} +  a_{23} \dx x_{21} & a_{13} \dx x_{12} +  a_{23} \dx x_{22} & a_{13} \dx x_{13} +  a_{23} \dx x_{23} \end{pmatrix}.
\]

\index{Когда мы берём её след, остаётся сумма элементов по диагонали. Это и есть требуемый дифференциал.} Дальше мы периодически будем пользоваться таким приёмом. Например,  величину 

\[ 
||X-A||^2 = \sum_{i,j} (x_{ij} - a_{ij})^2
\]

можно записать в матричном виде как 

\[
\tr((X-A)^T (X-A)).
\]

Итак, найдём производную от $f(X) = a^TXAXa$. Нам нужно выписать дифференциал и привести его к каноническому виду

Обратим внимание на то, что мы бьём из матриц в скаляры. Дифференциал будет по своей размерности совпадать со скаляром. Производная будет размера матрицы

\[
\dx{f(X)} = \dx{(a^TXAXa)} = \underset{[1 \times 1]}{a^T\dx{(X)}AXa} + \underset{[1 \times 1]}{a^TXA\dx{(X)}a}.
\]

Оба слагаемых, которые мы получаем после перехода к дифференциалу --- скаляры. Мы хотим представить дифференциал в виде $\tr(\text{нечто} \dx{X})$. След от скаляра это снова скаляр. Получается, что мы бесплатно можем навесить над правой частью наешго равенство знак следа и воспользоваться его свойствами

\begin{multline*}
\dx{f(X)} = \dx{(a^TXAXa)} = \tr(a^T\dx{(X)}AXa) + \tr(a^TXA\dx{(X)}a) = \\ = \tr(AXaa^T\dx{(X)}) + \tr(aa^TXA\dx{(X)}) = \\ = \tr(AXaa^T\dx{(X)} + aa^TXA\dx{(X)}) = \tr((AXaa^T + aa^TXA)\dx{(X)}).
\end{multline*}

Производная найдена, оказалось что это 

\[
\nabla f = (AXaa^T + aa^TXA)^T = aa^TX^TA^T + A^TXaa^T.
\]

Как бы мы нашли это, всё по-честному перемножив, даже боюсь себе представлять. 

\item Ещё один пример на ситуацию, когда функция бьёт из векторов в вектора

\[ 
\underset{[n \times 1]}{f(x)} = \underset{[n \times 1]}{x} \underset{[1 \times n]}{x^T}  \underset{[n \times 1]}{x}.
\]
В нём надо аккуратно вести себя со сложением матриц со скалярами. Берём дифференциал 

\[
\dx{f(x)} = \dx{xx^Tx} = \dx{x}x^Tx + x \dx{x^T} x + xx^T\dx{x}.
\]

В первом слагаемом пользуемся тем, что $x^Tx$ скаляр и его можно вынести перед дифференциалом. Этот скаляр умножается на каждый элемент вектора. Дальше мы захотим вынести дифференциал за скобку, чтобы не испортить матричное сложение, подчеркнём факт этого перемножения на каждый элемент единичной матрицей. Во втором слагаемом пользуемся тем, что $\dx{x^T} x$ скаляр и транспонируем его 
\[
\dx{f(x)} = \underset{[1 \times 1]}{x^Tx} \underset{[n \times n]}{I_n} \underset{[n \times 1]}{\dx{x}} + x x^T \dx{x} + xx^T\dx{x} = (x^Tx I_n + 2 x x^T)\dx{x}.
\]

Обратите внимание, что без единичной матрицы размерности у сложения поломаются. Получается, что наша производная выглядит как 

\[
\mathfrak{J} = x^Tx I_n + 2 x x^T = \begin{pmatrix} \sum x_i^2 + 2 x_1^2 & 2 x_1 x_2 & \ldots & 2 x_1 x_n \\
2 x_1 x_2 & \sum x_i^2 + 2 x_2^2 & \ldots & 2 x_2 x_n \\
\ldots & \ldots & \ddots &\ldots \\
2 x_1 x_n & 2 x_n x_2 & \ldots & \sum x_i^2 + 2 x_n^2 \\
\end{pmatrix}.
\]

Как и ожидалось, на выходе получилась матрица.


\item \indef{В нашей таблице. осталось ещё несколько ситуаций, которые остались вне поля нашего зрения.} Давайте их обсудим более подробно. Например, давайте посмотрим на ситуацию когда отображение бьёт из матриц в вектора

\[
f(X) : \RR^{n \times k} \to \RR^m.
\]

Тогда $X$ матрица, а $f(X)$ вектор. Нам надо найти производную каждого элемента из вектора $f(X)$ по каждому элементу из матрицы $X$. Получается, что $\frac{\partial f}{\partial X}$ --- это трёхмерная структура. Обычно в таких ситуациях ограничиваются записью частных производных либо прибегают к более сложным, многомерным методикам. Мы такие ситуации опустим. 
\end{enumerate} 
\end{sol}

\begin{problem}{}
Давайте пополним таблицу дифференциалов несколькими новыми функциями, специфичными для матриц. Найдём матричные дифференциалы функций: 
\begin{enumerate}
		\item $f(X) = X^{-1}$, где матрица $X$ размера $n \times n$
		
		\item  $f(X) = \det X$, где матрица $X$ размера $n \times n$
		
		\item $f(X) = \tr(X)$, где матрица $X$ размера $n \times n$
		
		\item Ещё больше матричных производных можно найти в книге The Matrix Cookbook\footnote{\url{https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf}}
	\end{enumerate}
\end{problem} 

\begin{sol} 
\begin{enumerate}
    \item Найдём производную обратной матрицы.  Тут логично вспомнить, что производная константы это ноль, обратная матрица определяется как $X^{-1} \cdot X = I_n$, а единичная матрица $I_n$ это как раз константа. Берём дифференциал с обеих сторон нашего равенства 
    
    \[
    \dx{X^{-1}} X + X^{-1} \dx{X} = \dx{I_n} = 0,
    \]
    
    отсюда получаем что 
    
    \[
    \dx{X^{-1}} = - X^{-1} \dx{X} X^{-1}.
    \]
    
    \item Определитель --- это функция, которая бьёт из матриц в скаляры. Его можно искать по-разному, один из способов --- разложение по строке
    
    \[
    \det X = \sum_{j = 1}^n x_{ij} (-1)^{i + j} M_{ij}, 
    \]
    
    где $M_{ij}$ --- дополнительный минор матрицы $X$. Берём дифференциал 
    
    \[
    \dx{(\det X)} = (-1)^{1 + 1} M_{11} \dx{x_{11}} + (-1)^{1 + 2} M_{12}  \dx{x_{12}} + \ldots  = \sum_{ij} A_{ji} \dx{x_{ij}} = \tr(A \dx{X}),
    \]
    
    где $A_{ij} = (-1)^{i +j} M_{ij},$ то есть $A$ --- матрица алгебраических дополнений. Трюк со следом аналогичен пункту \ref{tr_tr} из предыдущей задачи. Вспомним, что обратную матрицу можно получить отталкиваясь от алгебраических дополнений по формуле 
    
    \[
    X^{-1} = \frac{A}{\det X}.
    \]
    
    Выразим матрицу $A$ и подставим её а получившееся выше уравнение 
    
    \[
    \dx{(\det X)} = \tr(\det (X) X^{-1} \dx{X}),
    \]
    
    выходит что искомая производная равна 
    
    \[
    \nabla f = (\det X \cdot X^{-1})^T = \det X \cdot X^{-T}.
    \]
    
    \item По аналогии с определителем след бьёт из пространства матриц в пространство скаляров, получается 
    
    \[
    \dx{(\tr X)} = \tr(I_n \dx X).
    \]
    
    Это логично, так как след представляет из себя сумму диагональных элементов.
\end{enumerate} 
\end{sol}


% \begin{problem}{}
% \todo[inline]{Задача про матричную экспоненту и её производную}
% \end{problem}


%%%-------------------------------------------
\begin{problem}{}
	Найдите следующие производные:
	\begin{enumerate}
		\item $f(X) = \tr(AXB)$, где матрица $A$ размера $p \times m$, матрица $B$ размера $n \times p$, матрица $X$ размера $m \times n$.  % A^TB^T
		
		\item $f(X) = \tr(AX^TX)$, где матрица $A$ размера $n \times n$, матрица $X$ размера $m \times n$.  % XA + XA^T
		
		\item $f(X) = \ln \det X$ % X^{-1}
		
		\item $f(X) = \tr(AX^TXBX^{-T})$
		
		\item $f(X) = \det(X^TAX)$
		
		\item $f(x) = x^TAb$, где матрица $A$ размера $n \times n$, вектора $x$ и $b$ размера $n \times 1$. 
		
		\item $f(A) = x^TAb$. 
	\end{enumerate}
\end{problem}
\begin{sol} 
Проверить правильность своего решения можно в матричном калькуляторе\footnote{ \url{http://www.matrixcalculus.org/}}. Не забывайте, что $\tr(A) = \tr(A^T)$ и что под знаком следа можно циклически переставлять матрицы, если размерность не ломается. 
\end{sol} 


%%%-------------------------------------------
\begin{problem}{}
	Рассмотрим задачу линейной регресии
	\[
	L(w) = (y - Xw)^T(y - Xw) \to \min_{w}.
	\]
	
	\begin{enumerate}
		\item Найдите $L(w)$, выведите формулу для оптимального $w$.
		\item Как выглядит шаг градиентного спуска в матричном виде?
		\item Найдите $\dx^2L(w)$. Убедитесь, что мы действительно в точке минимума. 
	\end{enumerate}
\end{problem}
\begin{sol} 
Ради интереса убедимся, что перед нами в качестве функции потерь используется именно MSE, в качестве $x_i$ будем обозначать $i-$ую строчку матрицы $X$

\[
(y - Xw)^T(y - Xw) = \begin{pmatrix} y_1 - x_1^Tw &  \ldots & y_n - x_n^Tw \end{pmatrix} \begin{pmatrix} y_1 - x_1^Tw \\  \ldots \\ y_n - x_n^Tw \end{pmatrix}   = \sum_{i=1}^n (y_i - x_i^Tw)^2.
\]

Найдём дифференциал для нашей функции потерь, держим в голове что производная берётся по вектору $w$ 

\begin{multline*}
\dx L = \dx[(y - Xw)^T(y - Xw)] = \dx[(y - Xw)^T] (y - Xw) + (y - Xw)^T\dx[(y - Xw)] = \\ = \dx[(-Xw)^T] (y - Xw) - (y - Xw)^TX\dx w = \\ = - \dx w^T X^T (y - Xw) - (y - Xw)^TX\dx w = -2 (y - Xw)^TX \dx w.
\end{multline*} 

Тут мы воспользовались тем, что $ \dx w^T X^T (y - Xw)$ это скаляр и его можно транспонировать. Производная найдена. Шаг градиентного спуска будет выглядеть как 

\[
w_t = w_{t-1} + \gamma \cdot 2X^T(y-Xw).
\]

Здесь $\gamma$ --- это скорость обучения. Приравняем производную к нулю, чтобы найти минимум для $w$. Получается система уравнений

\[
2X^T(y-Xw) = 0 \qquad  X^Ty = X^TX w \qquad w = (X^TX)^{-1} X^Ty.
\]

При решении системы мы сделали предположение, что матрица $X^TX$ обратима. Это так, если в матрице $X$ нет линейно зависимых столбцов, а также наблюдений больше чем переменных. Найдём вторую производную 

\[
\dx[-2X^T(y-Xw)] = 2X^TX \dx w.
\]

Выходит, что $H = 2X^TX$. Так как матрица $X^TX$ положительно определена, по критерию Сильвестра, мы находимся в точке минимума. 

Матрица $X^TX$ положительно определена по определению. Если для любого вектора $v \ne 0$ квадратичная форма $v^T X^TX v > 0$, матрица  $X^TX$ положительно определена. При перемножении $Xv$ у нас получается вектор. Обозначим его как $z$, значит $v^T X^TX v = z^T z = \sum_{i=1}^n z_i^2 > 0$. 

Выпишем в явном виде второй дифференциал 

\[
\dx^2 L = \dx w^T 2X^TX \dx w.
\]
\end{sol} 

%%%-------------------------------------------
\begin{problem}{}
    В случае Ridge-регрессии минимизируется функция со штрафом:
	\[
	L(w) = (y - xw)^T(y - xw) + \lambda w^2,
	\]
	где $\lambda$ — положительный параметр, штрафующий функцию за слишком большие значения $w$.
	
	\begin{enumerate}
		\item Найдите $dL(w)$, выведите формулу для оптимального $w$.
		\item Как выглядит шаг градиентного спуска в матричном виде?
		\item Найдите $d^2L(w)$. Убедитесь, что мы действительно в точке минимума. 
	\end{enumerate}
	
% 	В случае Lasso-регрессии минимизируется функция со штрафом:
% 	\[
% 	L(w) = (y - xw)^T(y - xw) + \lambda |w|,
% 	\]
	
% 	Найдите решение этой задачи в явном виде. Как будет выглядеть шаг градиентного спуска для неё? 
\end{problem}
\begin{sol} 
\begin{equation*} 
\begin{aligned} 
& \dx L = 2 (y - Xw)^TX \dx w + 2\lambda w^T \dx{w} \\ 
& \nabla L(w) = 2 X^T (Xw - y) + 2\lambda w \\
& w = (X^TX + \lambda I)^{-1} X^Ty \\
& w_t = w_{t-1} - \gamma \cdot (2 X^T (Xw_{t-1} - y) + 2\lambda w_{t-1}) \\
& \dx^2 L = \dx{w^T}(2X^TX + 2\lambda)\dx{w} \\
& H = 2X^TX + 2\lambda \text{ положительно определена}
\end{aligned} 
\end{equation*} 

% Для Lasso-модели нужно рассмотреть два случая: 

% \begin{itemize} 
% \item $w \ge 0: \quad L(w) - (y - xw)^T(y-xw) + \lambda w$. Решив, получаем оптимальное значение 

% \[ w^{+} = \frac{x^Ty}{x^Tx} - \frac{\lambda}{2x^Tx}.\]

% \item $w < 0: \quad L(w) - (y - xw)^T(y-xw) - \lambda w$. Решив, получаем оптимальное значение 

% \[ w^{-} = \frac{x^Ty}{x^Tx} + \frac{\lambda}{2x^Tx}.\]
% \end{itemize} 

% Заметим, что $L(w)$ --- это парабола. Рассмотрим четыре ситуации положения $w^{+}$ и $w^{-}$ и получим ответ: 

% \begin{equation*} 
% \begin{aligned} 
% & w^{+} < 0, w^{-1} < 0 \Rightarrow \hat{w} = w^{-} \\
% & w^{+} > 0, w^{-1} > 0 \Rightarrow \hat{w} = w^{+} \\
% & w^{+} < 0, w^{-1} > 0 \Rightarrow \hat{w} = 0 \\
% & w^{+} > 0, w^{-1} < 0  \text{ этот случай невозможен.}
% \end{alligned} 
% \end{equation*} 
\end{sol}


%%%-------------------------------------------
\begin{problem}{}
	Пусть $x_i$ — вектор-столбец $k\times 1$, $y_i$ — скаляр, равный $+1$ или $-1$, $w$ — вектор-столбец размера $k\times 1$. Рассмотрим логистическую функцию потерь с $l_2$ регуляризацией
	\[
	L(w) = \sum_{i=1}^n \ln (1 + \exp(-y_ix_i^Tw)) + \lambda w^T w
	\]
	
	\begin{enumerate}
		\item Найдите $\dx L$;
		\item Найдите вектор-столбец $\nabla L$.
		\item Как для этой функции потерь выглядит шаг градиентного спуска в матричном виде? 
	\end{enumerate}
\end{problem}
\begin{sol} 
Используем весь арсенал, который обсудили выше. Начнём с одного слагаемого. Обозначим его как $y$. Это скаляр, значит

\[
\dx \ln y = \frac{1}{y} \dx y =  \frac{1}{\ln (1 + \exp(-y_ix_i^Tw))} \cdot -y_i \exp(-y_i x_i^T w) \cdot x_i^T \dx w.
\]

Выписываем дифференциал

\[
\dx L = \left( - \sum_{i=1}^n \frac{y_i \exp(-y_i x_i^T w)}{1 + \exp(-y_ix_i^Tw)} \cdot x_i^T + 2 \lambda w^T  \right) \dx{w}.
\]

Можно записать градиент с помощью сигмоиды $\sigma(z) = \frac{1}{1 + \exp(-z)}.$ Получится, что 

\[
\nabla L = \sum_{i=1}^n -y_i \sigma(-y_i x_i^T w) x_i + 2 \lambda w.
\]

Выходит, что шаг градиентного спуска можно записать как 

\[
w_t = w_{t-1} + \gamma \cdot \nabla L.
\]
\end{sol} 


%%%-------------------------------------------
\begin{problem}{}
	Упражняемся в матричном методе максимального правдоподобия.  Допустим, что выборка размера $n$ пришла к нам из многомерного нормального распределения с неизвестными вектором средних $\mu$ и ковариационной матрицей $\Sigma$. В этом задании нужно найти оценки максимального правдоподобия для $\hat \mu$ и $\hat \Sigma$.  Обратите внимание, что выборкой здесь будет не $x_1, \ldots, x_n$, а 
	\begin{equation*}
	\begin{pmatrix}
	x_{11}, \ldots, x_{n1} \\
	\ldots  \\ 
	x_{n1}, \ldots, x_{nm}
	\end{pmatrix}
	\end{equation*}
\end{problem}
\begin{sol} 

Плотность распределения для $m-$мерного вектора $y$ будет выглядеть как 
	
	\[ 
	f(x \mid \mu, \Sigma) = \frac{1}{(\sqrt{2 \pi})^m \cdot \sqrt{\det \Sigma}} \cdot \exp \left( - \frac{1}{2} \cdot (x - \mu)^T \Sigma^{-1} (x - \mu) \right).
	\]
	
	В силу того, что все наблюдения независимы, функция правдоподобия для выборки объёма $n$ примет вид: 
	
	\[
	L(x \mid \mu, \Sigma) = \frac{1}{(\sqrt{2 \pi})^{m \cdot n} \cdot \sqrt{\det \Sigma}^n} \cdot \exp \left( - \frac{1}{2} \cdot \sum_{i = 1}^n (x_i - \mu)^T \Sigma^{-1} (x_i - \mu) \right).
	\]
	
	Прологарифмировав правдоподобие, получим
	
	\[
	\ln L(x \mid \mu, \Sigma) = - \frac{m \cdot n}{2} \ln 2 \pi - \frac{n}{2} \ln \det \Sigma - \frac{1}{2} \sum_{i=1}^n (x_i - \mu)^T \Sigma^{-1} (x_i - \mu) 
	\]
	
	Нам нужно найти максимум этой функции по $\mu$  и $\Sigma$.  Начнём с $\mu$.  Аргумент $\Sigma$ будем считать константой. Обозначим такую функцию за $f(\mu)$. Эта функция бьёт с множества векторов в множество скаляров.  Значит дифференциал этой функции можно записать в виде: 
	
	\[ df(\mu) = \nabla f^T d \mu. \]
	
	Найдём этот дифференциал. Не будем забывать, что дифференциал от константы нулевой, а также что дифференциал суммы равен сумме дифференциалов
	
	\begin{multline*}
	d f(\mu) = -\frac{1}{2} \cdot d \sum_{i=1}^n (x_i - \mu)^T \Sigma^{-1} (x_i - \mu) = -\frac{1}{2} \cdot \sum_{i=1}^n d[(x_i - \mu)^T \Sigma^{-1} (x_i - \mu)] = \\ = -\frac{1}{2} \cdot \sum_{i=1}^n d[(x_i - \mu)^T] \Sigma^{-1} (x_i - \mu) + (x_i - \mu)^T \Sigma^{-1} d[(x_i - \mu)] = \\ =   \frac{1}{2} \cdot \sum_{i=1}^n d\mu^T \Sigma^{-1} (x_i - \mu) + (x_i - \mu)^T \Sigma^{-1} d\mu.
	\end{multline*}
	
	Первое слагаемое под суммой имеет размерность $1 \times m \cdot m \times m \cdot m \times 1$. Это константа. Если мы протранспонируем константу, ничего не изменится. Обратим внимание, что матрица $\Sigma$ симметричная и при транспонировании не меняется. Сделаем этот трюк 
	
	\begin{multline*}
	 \frac{1}{2} \cdot \sum_{i=1}^n d\mu^T \Sigma^{-1} (x_i - \mu) + (x_i - \mu)^T \Sigma^{-1} d\mu = \frac{1}{2} \cdot \sum_{i=1}^n  (x_i - \mu)^T \Sigma^{-1}  d\mu  + (x_i - \mu)^T \Sigma^{-1} d\mu = \\ = \frac{1}{2} \cdot \sum_{i=1}^n  [(x_i - \mu)^T \Sigma^{-1}  + (x_i - \mu)^T \Sigma^{-1} ] d\mu =  \left[  \cdot \sum_{i=1}^n  (x_i - \mu)^T \Sigma^{-1} \right] d\mu 
	\end{multline*}	
	
	Получается, что $f'(\mu) = \sum_{i=1}^n   \Sigma^{-1} (x_i - \mu)$.  Приравняв производную к нулю и домножив обе части уравнения слева на $\Sigma$, получим оптимальное значению $\mu$: 
	
	\begin{equation*}
	\begin{aligned}
	&\sum_{i=1}^n   \Sigma^{-1} (x_i - \hat \mu) = 0 \\
	&\sum_{i=1}^n   (x_i - \hat \mu) = 0 \\
	&\sum_{i=1}^n   x_i =  n \cdot \hat \mu \Rightarrow \hat \mu = \bar x.
	\end{aligned} 
	\end{equation*}
		
Не будем забывать, что в записях выше $x$ и $\mu$ были векторами-столбцами размерности $m \times 1$. В итоговом ответе они также являются векторами-столбцами такой размерности. 

Займёмся оценкой для $\Sigma.$ Аргумент $\mu$ будем считать константой. Обозначим такую функцию за $f(\Sigma)$

\[
f(\Sigma) = - \frac{n}{2} \ln \det \Sigma - \frac{1}{2} \sum_{i=1}^n (x_i - \mu)^T \Sigma^{-1} (x_i - \mu).
\]

Эта функция бьёт с множества матриц в множество скаляров. Значит дифференциал этой функции можно записать в виде: 

\[
d f(\Sigma) = \tr (\nabla f^T dx).
\]

Начнём с первого слагаемого. Для него нам понадобится вспомнить как выглядит дифференциал для определителя

\[
- \frac{n}{2} \frac{1}{\det \Sigma}  d[\det \Sigma]  = - \frac{n}{2} \frac{1}{\det \Sigma}  \tr( \det \Sigma \cdot \Sigma^{-T} d \Sigma) =   -  \tr( \frac{n}{2} \cdot \Sigma^{-1} d \Sigma).
\]

Теперь поработаем со вторым слагаемым. В нём нас интересует дифференциал обратной матрицы

\[
- \frac{1}{2} \sum_{i=1}^n (x_i - \mu)^T d[\Sigma^{-1}] (x_i - \mu) = \frac{1}{2} \sum_{i=1}^n (x_i - \mu)^T \Sigma^{-1} \cdot  d \Sigma \cdot \Sigma^{-1} (x_i - \mu). 
\]	
	
Под знаком суммы размерность каждого слагаемого $1 \times m \cdot m \times m \cdot m \times m \cdot m \times m \cdot m \times 1$. Это константа. Если мы возьмём от неё след, ничего не изменится. Взяв след, переставим внутри множители

\[
\frac{1}{2} \sum_{i=1}^n (x_i - \mu)^T \Sigma^{-1} \cdot  d \Sigma \cdot \Sigma^{-1} (x_i - \mu)  = \frac{1}{2} \sum_{i=1}^n \tr( \Sigma^{-1} (x_i - \mu) \cdot (x_i - \mu)^T \Sigma^{-1} \cdot  d \Sigma). 
\] 

Сумма следов --- след суммы. Объединяем наши слагаемые в месте. В первом множитель $n$ подменяем на сумму 

\[
d f(\Sigma) = \tr \left( \left [ - \frac{1}{2} \sum_{i = 1}^n \Sigma^{-1} + \Sigma^{-1} (x_i - \mu) \cdot (x_i - \mu)^T \Sigma^{-1} \right] d \Sigma \right)
\]

Забираем себе из-под знака дифференциала производную. Под знаком суммы после транспонирования ничего не поменяется. Приравниваем производную к нулю, домножим справа каждое слагаемое на $\Sigma$. На четвёртой строчке домножим слева на $\Sigma$: 

\begin{equation*}
\begin{aligned} 
 & \frac{1}{2} \sum_{i = 1}^n - \Sigma^{-1} + \Sigma^{-1} (x_i - \mu) \cdot (x_i - \mu)^T \Sigma^{-1}  = 0 \\
 & - n \cdot \Sigma^{-1} + \sum_{i = 1}^n  \Sigma^{-1} (x_i - \mu) \cdot (x_i - \mu)^T \Sigma^{-1}  = 0 \\
 & - n + \Sigma^{-1} \sum_{i = 1}^n  (x_i - \mu) \cdot (x_i - \mu)^T = 0 \\
 & - n \Sigma+ \sum_{i = 1}^n  (x_i - \mu) \cdot (x_i - \mu)^T = 0 \\
 &  \Sigma  = \frac{1}{n} \sum_{i = 1}^n  (x_i - \mu) \cdot (x_i - \mu)^T 
\end{aligned}
\end{equation*}
	
До оценок остался один шаг. Вспоминаем оценку для $\mu$, подставляем её в уравнение и получаем, что 

\[
\hat \Sigma = \frac{1}{n} \sum_{i = 1}^n  (x_i - \bar x) \cdot (x_i - \bar x)^T.
\]
	
Не забываем, что $x_i$ и $\bar x$ --- вектора размерности $m \times 1$. 
\end{sol} 


%%%-------------------------------------------
\begin{problem}{}
	Найдите симметричную матрицу $X$ наиболее близкую к матрице $A$ по норме Фробениуса, $\sum_{i,j} (x_{ij} - a_{ij})^2$. Тут мы просто из каждого элемента вычитаем каждый и смотрим на сумму квадратов таких разностей. То есть решите задачку условной матричной минимизации 
	
	\begin{equation*}
	\begin{cases}
	& ||X - A||^2 \to \min_{A}  \\
	& X^T = X
	\end{cases}
	\end{equation*}
	
	\textbf{Hint:} Надо будет выписать Лагранджиан.  А ещё пригодится тот факт, что $\sum_{i,j} (x_{ij} - a_{ij})^2 = ||X-A||^2 =  \tr((X-A)^T (X-A))$.
\end{problem}

\begin{sol} 
Выписываем лагранджиан

\begin{multline*}
\mathscr{L} = \sum_{i,j} (x_{ij} - a_{ij})^2 + \sum_{ij} \lambda_{ij} (x_{ij} - x_{ji}) = \tr((X-A)^T (X-A)) + \tr(\Lambda^T (X - X^T)) = \\ = \tr(X^TX) - 2 \tr(X^TA) + \tr(A^TA) + \tr(\Lambda^T (X - X^T))
\end{multline*}

Найдём все необходимые нам дифференциалы

\begin{equation*}
    \begin{aligned} 
    & \dx \tr(X^TX) = \tr(\dx(X^TX))  = \tr(X^T\dx X) + \tr(\dx X^T X) = \tr(2 X^T\dx X) \\
    & \dx \tr(X^TA) = \tr(A^T \dx{X}) \\
    & \dx \tr(\Lambda^TX) = \tr(\Lambda^T \dx{X}) \\
    & \dx \tr(\Lambda^TX^T) = \tr(\Lambda \dx{X}) \\
    \end{aligned} 
\end{equation*}

Выписываем в яном виде производную по $X$ 

\[
\frac{\partial \mathscr{L}}{\partial X} = 2X^T - 2A^T + \Lambda^T - \Lambda = 0
\]

Нужно избавиться от $\Lambda$, давайте транспонируем уравнение

\[
\frac{\partial \mathscr{L}}{\partial X} = 2X - 2A + \Lambda - \Lambda^T = 0,
\]

а после прибавим его к исходному, тогда лишние части исчезнут 

\[
4X - 2A^T - 2A = 0 \qquad X = \frac{1}{2}(A + A^T).
\]
\end{sol} 

% \todo[inline]{Придумать ещё какую-нибудь задачу оптимизации} 
% в мидтерме есть с матрицей обратной 
